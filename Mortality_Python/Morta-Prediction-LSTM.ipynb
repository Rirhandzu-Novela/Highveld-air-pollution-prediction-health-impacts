{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c0d27-a8ae-4003-8736-e420f852dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from sklearn.metrics import r2_score \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "%matplotlib inline\n",
    "#%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "#print(tf.__version_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the dataset\n",
    "df = pd.read_csv('C:/Users/User/Documents/GitHub/Health-impacts-of-air-pollution/MortData/GertPollCardMort.csv', sep=';', header=0, index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b147c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any NaN values\n",
    "df_cleaned = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b09306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target variable (mortality) and use everything else as features\n",
    "target = 'death_count'\n",
    "features = df_cleaned.drop(columns=[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe3a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features and the target separately\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(df_cleaned[[target]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the Data for LSTM\n",
    "def create_sequences(data, target_data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X.append(data[i:i + n_steps])\n",
    "        y.append(target_data[i + n_steps])  # Predict next day's mortality\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "n_steps = 30  # 30 days look-back for daily data\n",
    "X, y = create_sequences(scaled_features, scaled_target, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5de404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to fit LSTM input format (samples, time steps, features)\n",
    "X = X.reshape((X.shape[0], X.shape[1], X.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5fa535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab81c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build the LSTM Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(n_steps, X.shape[2])))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e198b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the Model\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f93e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Make Predictions for Daily Mortality\n",
    "daily_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac8415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform the predictions to the original mortality scale\n",
    "daily_predictions_rescaled = scaler_target.inverse_transform(daily_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4127cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Aggregate Daily Predictions to Monthly Predictions\n",
    "df_predictions = pd.DataFrame({'date': df_cleaned.index[-len(daily_predictions_rescaled):], 'predicted_mortality': daily_predictions_rescaled.flatten()})\n",
    "df_predictions.set_index('date', inplace=True)\n",
    "\n",
    "# Sum daily predictions by month\n",
    "monthly_predictions = df_predictions.resample('M').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1dc80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform y_test to get the actual mortality values for the test set\n",
    "y_test_rescaled = scaler_target.inverse_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54426796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the date range corresponding to the test set\n",
    "test_dates = df_cleaned.index[-len(y_test_rescaled):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for y_test with the appropriate date index\n",
    "df_actual_test = pd.DataFrame({\n",
    "    'actual_mortality': y_test_rescaled.flatten()\n",
    "}, index=test_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98bd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the actual test values by month to get monthly sums\n",
    "df_actual_monthly = df_actual_test.resample('M').sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761c6e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['font.weight'] = 'bold'\n",
    "rcParams['font.size'] = '15'\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot actual vs predicted monthly mortality\n",
    "plt.plot(df_actual_monthly.index, df_actual_monthly.values, label='Actual Monthly Mortality')\n",
    "plt.plot(monthly_predictions.index, monthly_predictions['predicted_mortality'].values, label='Predicted Monthly Mortality')\n",
    "\n",
    "# Customize the x-axis labels\n",
    "#plt.xticks(rotation=90)\n",
    "\n",
    "plt.ylabel('Mortality', fontname=\"Times New Roman\", size=30, fontweight=\"bold\")\n",
    "plt.xlabel('Date', fontname=\"Times New Roman\", size=30, fontweight=\"bold\")\n",
    "plt.title('Nkangala cardiovascular mortality LSTM', fontname=\"Times New Roman\", size=28, fontweight=\"bold\")\n",
    "\n",
    "# Set legend properties\n",
    "legend_properties = {'weight': 'bold'}\n",
    "plt.legend(prop=legend_properties)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b848c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(df_actual_monthly, monthly_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815716c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = sqrt(mean_squared_error(df_actual_monthly, monthly_predictions))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975542d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(df_actual_monthly, monthly_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1eacb",
   "metadata": {},
   "source": [
    "# LSTM FOR SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf5e12-dba4-4ee3-be81-d6a9131e721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Sample a subset of the test set (SHAP is computationally expensive)\n",
    "#X_test_sample = X_test[:10]  # Use a small sample for demonstration (First 10)\n",
    "X_test_sample = X_test[np.random.choice(X_test.shape[0], 100, replace=False)] #(random 10)\n",
    "\n",
    "# Reshape the 3D data into 2D for SHAP (flatten time steps and features)\n",
    "X_train_flattened = X_train.reshape((X_train.shape[0], X_train.shape[1] * X_train.shape[2]))\n",
    "X_test_sample_flattened = X_test_sample.reshape((X_test_sample.shape[0], X_test_sample.shape[1] * X_test_sample.shape[2]))\n",
    "\n",
    "# Create a SHAP KernelExplainer using the model's prediction function\n",
    "explainer = shap.KernelExplainer(\n",
    "    lambda x: model.predict(x.reshape((x.shape[0], n_steps, X.shape[2]))).reshape(-1),  # Ensure the output is 1D\n",
    "    X_train_flattened[:100]\n",
    ")\n",
    "\n",
    "# Calculate SHAP values for the test sample\n",
    "shap_values = explainer.shap_values(X_test_sample_flattened)\n",
    "\n",
    "# Feature names (flattened time steps and features)\n",
    "flattened_feature_names = [f\"{feature}_timestep_{i}\" for i in range(n_steps) for feature in features.columns]\n",
    "\n",
    "# Ensure the length of feature names matches the number of features\n",
    "assert len(flattened_feature_names) == X_test_sample_flattened.shape[1], \"Feature names length mismatch.\"\n",
    "\n",
    "# Plot summary plot of SHAP values\n",
    "shap.summary_plot(shap_values, X_test_sample_flattened, feature_names=flattened_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f586ee4",
   "metadata": {},
   "source": [
    "# LSTM WITH ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input, Flatten, Dot, Softmax\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the attention mechanism\n",
    "def attention_layer(inputs):\n",
    "    query = Dense(1, use_bias=False)(inputs)  # Query layer\n",
    "    keys = Dense(1, use_bias=False)(inputs)   # Key layer\n",
    "    values = Dense(1, use_bias=False)(inputs) # Value layer\n",
    "\n",
    "    # Compute attention scores\n",
    "    scores = Dot(axes=[2, 2])([query, keys])  # Shape: (batch_size, n_steps, 1)\n",
    "    scores = Softmax()(scores)  # Normalize scores\n",
    "    context_vector = Dot(axes=[1, 1])([scores, values])  # Shape: (batch_size, 1, features)\n",
    "\n",
    "    return context_vector, scores\n",
    "\n",
    "# Define the model\n",
    "def build_model(n_steps, n_features):\n",
    "    inputs = Input(shape=(n_steps, n_features))\n",
    "    x = LSTM(50, return_sequences=True)(inputs)\n",
    "    context_vector, attention_scores = attention_layer(x)  # Apply attention mechanism and capture scores\n",
    "    x = Flatten()(context_vector)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    outputs = Dense(1)(x)  # Output is just the prediction\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model, attention_scores  # Return attention scores for later use\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv('NkaPollCardMort.csv', sep=';', header=0, index_col=0, parse_dates=True)\n",
    "df_cleaned = df.dropna()\n",
    "target = 'death_count'\n",
    "features = df_cleaned.drop(columns=[target])\n",
    "\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(df_cleaned[[target]])\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, target_data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X.append(data[i:i + n_steps])\n",
    "        y.append(target_data[i + n_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "n_steps = 30\n",
    "X, y = create_sequences(scaled_features, scaled_target, n_steps)\n",
    "X = X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Build and train the model\n",
    "model, attention_scores = build_model(n_steps, X.shape[2])\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Rescale the predictions\n",
    "daily_predictions_rescaled = scaler_target.inverse_transform(predictions)\n",
    "\n",
    "# Evaluate the model to get attention scores\n",
    "example_input = X_test[:1]  # Use a sample from your test data\n",
    "attention_model = tf.keras.models.Model(inputs=model.input, outputs=attention_scores)\n",
    "attention_scores_sample = attention_model.predict(example_input)\n",
    "\n",
    "# Plot the attention scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(attention_scores_sample[0], aspect='auto', cmap='viridis')\n",
    "plt.colorbar(label='Attention Score')\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Attention Scores')\n",
    "plt.title('Attention Scores Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a9d6c",
   "metadata": {},
   "source": [
    "Interpreting the attention scores plot is a great way to understand how your model is focusing on different time steps. Here’s how you can make sense of it:\n",
    "\n",
    "Heatmap Colors: The colors on the heatmap represent the attention scores, ranging from low to high values. Typically, a color gradient (like yellow to purple in viridis colormap) indicates this range. Brighter colors (closer to yellow) suggest higher attention scores, meaning the model is paying more attention to those time steps. Darker colors (closer to purple) indicate lower attention scores.\n",
    "\n",
    "X-axis (Timesteps): This axis represents the sequence of time steps in your input data. For example, if you’re using 30 time steps in each input sequence, the x-axis will range from 1 to 30.\n",
    "\n",
    "Y-axis (Attention Scores): This axis shows the attention scores corresponding to each time step. Since you used a sample input from your test data, this visualizes how much importance the model assigns to each of the 30 time steps for that specific sample.\n",
    "\n",
    "Understanding Model Focus: By looking at the attention scores, you can determine which time steps had the most significant impact on the model’s prediction. High attention scores at certain timesteps suggest those periods are particularly informative or influential in making the prediction.\n",
    "\n",
    "Practical Insight:\n",
    "Clusters of High Attention: If you see clusters of bright colors, it implies that the model is focusing on specific periods in your time series data.\n",
    "\n",
    "Consistent High Attention: If high attention is spread evenly across several time steps, it suggests that the model considers all these steps equally important for making predictions.\n",
    "\n",
    "Patterns: Look for patterns in the attention scores to see if the model’s focus changes over time in a meaningful way that correlates with known events or trends in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d1ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b754247",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d34659e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
